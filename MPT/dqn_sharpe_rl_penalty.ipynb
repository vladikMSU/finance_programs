
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN для выбора лимитов портфеля с максимизацией Sharpe Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка",
    "!pip install numpy matplotlib torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт",
    "import numpy as np",
    "import torch",
    "import torch.nn as nn",
    "import torch.optim as optim",
    "import random",
    "import matplotlib.pyplot as plt",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс среды",
    "class SharpeEnv:",
    "    def __init__(self):",
    "        self.state = np.array([0.1, 0.1, 0.1])  # crypto, stocks_ru, bonds_ru",
    "        self.step_size = 0.01",
    "        self.bounds = [0.0, 0.3]",
    "        self.action_space = 6",
    "",
    "    def reset(self):",
    "        self.state = np.random.uniform(0.0, 0.3, size=3)",
    "        return self.state",
    "",
    "    def step(self, action):",
    "        idx = action // 2",
    "        delta = self.step_size if action % 2 == 0 else -self.step_size",
    "        self.state[idx] = np.clip(self.state[idx] + delta, *self.bounds)",
    "        sharpe = -np.sum((self.state - 0.2)**2) + 1.0  # Заглушка Sharpe",
    "        reward = sharpe",
    "        return self.state.copy(), reward, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN модель",
    "class DQN(nn.Module):",
    "    def __init__(self, state_dim, action_dim):",
    "        super(DQN, self).__init__()",
    "        self.net = nn.Sequential(",
    "            nn.Linear(state_dim, 64),",
    "            nn.ReLU(),",
    "            nn.Linear(64, action_dim)",
    "        )",
    "    def forward(self, x):",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основной цикл обучения",
    "env = SharpeEnv()",
    "q_net = DQN(3, 6)",
    "target_net = DQN(3, 6)",
    "target_net.load_state_dict(q_net.state_dict())",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-3)",
    "buffer = deque(maxlen=10000)",
    "batch_size = 64",
    "gamma = 0.99",
    "epsilon = 0.2",
    "",
    "rewards_log = []",
    "",
    "for ep in range(300):",
    "    s = env.reset()",
    "    total_reward = 0",
    "    for _ in range(20):",
    "        if random.random() < epsilon:",
    "            a = random.randint(0, 5)",
    "        else:",
    "            with torch.no_grad():",
    "                a = q_net(torch.FloatTensor(s)).argmax().item()",
    "        s2, r, done, _ = env.step(a)",
    "        buffer.append((s, a, r, s2))",
    "        s = s2",
    "        total_reward += r",
    "",
    "        if len(buffer) >= batch_size:",
    "            batch = random.sample(buffer, batch_size)",
    "            states, actions, rewards, next_states = zip(*batch)",
    "            states = torch.FloatTensor(states)",
    "            actions = torch.LongTensor(actions)",
    "            rewards = torch.FloatTensor(rewards)",
    "            next_states = torch.FloatTensor(next_states)",
    "",
    "            q_vals = q_net(states).gather(1, actions.unsqueeze(1)).squeeze()",
    "            next_q = target_net(next_states).max(1)[0]",
    "            target = rewards + gamma * next_q",
    "",
    "            loss = nn.functional.smooth_l1_loss(q_vals, target.detach())",
    "            optimizer.zero_grad()",
    "            loss.backward()",
    "            optimizer.step()",
    "",
    "    if ep % 10 == 0:",
    "        target_net.load_state_dict(q_net.state_dict())",
    "    rewards_log.append(total_reward)",
    "",
    "plt.plot(rewards_log)",
    "plt.title(\"Sharpe (Reward) по эпизодам - DQN\")",
    "plt.xlabel(\"Эпизод\")",
    "plt.ylabel(\"Reward (Sharpe Proxy)\")",
    "plt.grid(True)",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
